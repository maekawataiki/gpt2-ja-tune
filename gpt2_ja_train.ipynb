{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2_ja_train.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maekawataiki/gpt2-ja-tune/blob/master/gpt2_ja_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84pQelzpGtd-",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Setup Environemnt\n",
        "!git clone https://github.com/maekawataiki/gpt2-ja-tune gpt2\n",
        "%cd /content/gpt2\n",
        "!pip install sentencepiece toposort"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVdJJf0tinkw",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Downlaod Model\n",
        "\n",
        "model_name = 'ja-117M' #@param ['ja-117M', '124M', '355M', '774M', '1558M']\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "def download_model(model):\n",
        "    subdir = os.path.join('models', model)\n",
        "    if not os.path.exists(subdir):\n",
        "        os.makedirs(subdir)\n",
        "    subdir = subdir.replace('\\\\','/') # needed for Windows\n",
        "\n",
        "    for filename in ['checkpoint','encoder.json','hparams.json','model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta', 'vocab.bpe']:\n",
        "\n",
        "        r = requests.get(\"https://storage.googleapis.com/gpt-2/\" + subdir + \"/\" + filename, stream=True)\n",
        "\n",
        "        with open(os.path.join(subdir, filename), 'wb') as f:\n",
        "            file_size = int(r.headers[\"content-length\"])\n",
        "            chunk_size = 1000\n",
        "            with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
        "                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
        "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(chunk_size)\n",
        "\n",
        "if model_name != 'ja-117M':\n",
        "  download_model(model_name)\n",
        "else:\n",
        "  !wget https://www.nama.ne.jp/models/ja-117M.tar.bz2\n",
        "  !tar xvfj ja-117M.tar.bz2 --directory models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdwVoH34IRXV",
        "colab_type": "text"
      },
      "source": [
        "## Train / Fine-tune Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzWAlPikIHaJ",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Import Dependencies\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import tqdm\n",
        "from tensorflow.core.protobuf import rewriter_config_pb2\n",
        "from google.colab import drive\n",
        "\n",
        "import model, sampling as sample, encoder\n",
        "from load_dataset import load_dataset, Sampler\n",
        "from accumulate import AccumulatingOptimizer\n",
        "import memory_saving_gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw0am-ZojYT-",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBX7pThe_23Q",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Upload Data\n",
        "#@markdown File, directory, or glob pattern (utf-8 text, or preencoded .npz files)\n",
        "\n",
        "uploaded = files.upload()\n",
        "file_names = list(uploaded.keys())\n",
        "filename = file_names[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z151GFXL2Rt",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Train Model\n",
        "\n",
        "CHECKPOINT_DIR = '/content/gdrive/My Drive/Models'\n",
        "SAMPLE_DIR = 'samples'\n",
        "\n",
        "parser = argparse.ArgumentParser(\n",
        "    description='Fine-tune GPT-2 on your custom dataset.',\n",
        "    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "\n",
        "parser.add_argument('--dataset', metavar='PATH', type=str, required=True, help='Input file, directory, or glob pattern (utf-8 text, or preencoded .npz files).')\n",
        "parser.add_argument('--model_name', metavar='MODEL', type=str, default='117M', help='Pretrained model name')\n",
        "parser.add_argument('--combine', metavar='CHARS', type=int, default=50000, help='Concatenate input files with <|endoftext|> separator into chunks of this minimum size')\n",
        "parser.add_argument('--encoding', type=str, default='utf-8', help='Set the encoding for reading and writing files.')\n",
        "\n",
        "parser.add_argument('--batch_size', metavar='SIZE', type=int, default=1, help='Batch size')\n",
        "parser.add_argument('--learning_rate', metavar='LR', type=float, default=0.00002, help='Learning rate for Adam')\n",
        "parser.add_argument('--accumulate_gradients', metavar='N', type=int, default=1, help='Accumulate gradients across N minibatches.')\n",
        "parser.add_argument('--memory_saving_gradients', default=False, action='store_true', help='Use gradient checkpointing to reduce vram usage.')\n",
        "parser.add_argument('--only_train_transformer_layers', default=False, action='store_true', help='Restrict training to the transformer blocks.')\n",
        "parser.add_argument('--optimizer', type=str, default='adam', help='Optimizer. <adam|sgd>.')\n",
        "parser.add_argument('--noise', type=float, default=0.0, help='Add noise to input training data to regularize against typos.')\n",
        "\n",
        "parser.add_argument('--top_k', type=int, default=40, help='K for top-k sampling.')\n",
        "parser.add_argument('--top_p', type=float, default=0.0, help='P for top-p sampling. Overrides top_k if set > 0.')\n",
        "\n",
        "parser.add_argument('--restore_from', type=str, default='latest', help='Either \"latest\", \"fresh\", or a path to a checkpoint file')\n",
        "parser.add_argument('--run_name', type=str, default='run1', help='Run id. Name of subdirectory in checkpoint/ and samples/')\n",
        "parser.add_argument('--sample_every', metavar='N', type=int, default=100, help='Generate samples every N steps')\n",
        "parser.add_argument('--sample_length', metavar='TOKENS', type=int, default=1023, help='Sample this many tokens')\n",
        "parser.add_argument('--sample_num', metavar='N', type=int, default=1, help='Generate this many samples')\n",
        "parser.add_argument('--save_every', metavar='N', type=int, default=1000, help='Write a checkpoint every N steps')\n",
        "\n",
        "parser.add_argument('--val_dataset', metavar='PATH', type=str, default=None, help='Dataset for validation loss, defaults to --dataset.')\n",
        "parser.add_argument('--val_batch_size', metavar='SIZE', type=int, default=2, help='Batch size for validation.')\n",
        "parser.add_argument('--val_batch_count', metavar='N', type=int, default=40, help='Number of batches for validation.')\n",
        "parser.add_argument('--val_every', metavar='STEPS', type=int, default=0, help='Calculate validation loss every STEPS steps.')\n",
        "\n",
        "\n",
        "def maketree(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "def randomize(context, hparams, p):\n",
        "    if p > 0:\n",
        "        mask = tf.random.uniform(shape=tf.shape(context)) < p\n",
        "        noise = tf.random.uniform(shape=tf.shape(context), minval=0, maxval=hparams.n_vocab, dtype=tf.int32)\n",
        "        return tf.where(mask, noise, context)\n",
        "    else:\n",
        "        return context\n",
        "\n",
        "\n",
        "def main(model_name, dataset):\n",
        "    args = parser.parse_args(['--model_name', model_name, '--dataset', dataset])\n",
        "    enc = encoder.get_encoder(args.model_name)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join('models', args.model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if args.sample_length > hparams.n_ctx:\n",
        "        raise ValueError(\n",
        "            \"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    if args.model_name == '345M':\n",
        "        args.memory_saving_gradients = True\n",
        "        if args.optimizer == 'adam':\n",
        "            args.only_train_transformer_layers = True\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF\n",
        "    tf.reset_default_graph()\n",
        "    with tf.Session(config=config) as sess:\n",
        "        context = tf.placeholder(tf.int32, [args.batch_size, None])\n",
        "        context_in = randomize(context, hparams, args.noise)\n",
        "        output = model.model(hparams=hparams, X=context_in)\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                labels=context[:, 1:], logits=output['logits'][:, :-1]))\n",
        "\n",
        "        if args.val_every > 0:\n",
        "            val_context = tf.placeholder(tf.int32, [args.val_batch_size, None])\n",
        "            val_output = model.model(hparams=hparams, X=val_context)\n",
        "            val_loss = tf.reduce_mean(\n",
        "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    labels=val_context[:, 1:], logits=val_output['logits'][:, :-1]))\n",
        "            val_loss_summary = tf.summary.scalar('val_loss', val_loss)\n",
        "\n",
        "\n",
        "        tf_sample = sample.sample_sequence(\n",
        "            hparams=hparams,\n",
        "            length=args.sample_length,\n",
        "            context=context,\n",
        "            batch_size=args.batch_size,\n",
        "            temperature=1.0,\n",
        "            top_k=args.top_k,\n",
        "            top_p=args.top_p)\n",
        "\n",
        "        all_vars = [v for v in tf.trainable_variables() if 'model' in v.name]\n",
        "        train_vars = [v for v in all_vars if '/h' in v.name] if args.only_train_transformer_layers else all_vars\n",
        "\n",
        "        if args.optimizer == 'adam':\n",
        "            opt = tf.train.AdamOptimizer(learning_rate=args.learning_rate)\n",
        "        elif args.optimizer == 'sgd':\n",
        "            opt = tf.train.GradientDescentOptimizer(learning_rate=args.learning_rate)\n",
        "        else:\n",
        "            exit('Bad optimizer:', args.optimizer)\n",
        "\n",
        "        if args.accumulate_gradients > 1:\n",
        "            if args.memory_saving_gradients:\n",
        "                exit(\"Memory saving gradients are not implemented for gradient accumulation yet.\")\n",
        "            opt = AccumulatingOptimizer(\n",
        "                opt=opt,\n",
        "                var_list=train_vars)\n",
        "            opt_reset = opt.reset()\n",
        "            opt_compute = opt.compute_gradients(loss)\n",
        "            opt_apply = opt.apply_gradients()\n",
        "            summary_loss = tf.summary.scalar('loss', opt_apply)\n",
        "        else:\n",
        "            if args.memory_saving_gradients:\n",
        "                opt_grads = memory_saving_gradients.gradients(loss, train_vars)\n",
        "            else:\n",
        "                opt_grads = tf.gradients(loss, train_vars)\n",
        "            opt_grads = list(zip(opt_grads, train_vars))\n",
        "            opt_apply = opt.apply_gradients(opt_grads)\n",
        "            summary_loss = tf.summary.scalar('loss', loss)\n",
        "\n",
        "        summary_lr = tf.summary.scalar('learning_rate', args.learning_rate)\n",
        "        summaries = tf.summary.merge([summary_lr, summary_loss])\n",
        "\n",
        "        summary_log = tf.summary.FileWriter(\n",
        "            os.path.join(CHECKPOINT_DIR, args.run_name))\n",
        "\n",
        "        saver = tf.train.Saver(\n",
        "            var_list=all_vars,\n",
        "            max_to_keep=2,\n",
        "            keep_checkpoint_every_n_hours=2)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        if args.restore_from == 'latest':\n",
        "            ckpt = tf.train.latest_checkpoint(\n",
        "                os.path.join(CHECKPOINT_DIR, args.run_name))\n",
        "            if ckpt is None:\n",
        "                # Get fresh GPT weights if new run.\n",
        "                ckpt = tf.train.latest_checkpoint(\n",
        "                    os.path.join('models', args.model_name))\n",
        "        elif args.restore_from == 'fresh':\n",
        "            ckpt = tf.train.latest_checkpoint(\n",
        "                os.path.join('models', args.model_name))\n",
        "        else:\n",
        "            ckpt = tf.train.latest_checkpoint(args.restore_from)\n",
        "        print('Loading checkpoint', ckpt)\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        print('Loading dataset...')\n",
        "        chunks = load_dataset(enc, args.dataset, args.combine, encoding=args.encoding)\n",
        "        # print(\"Chunk size: \")\n",
        "        data_sampler = Sampler(chunks)\n",
        "        if args.val_every > 0:\n",
        "            if args.val_dataset:\n",
        "                val_chunks = load_dataset(enc, args.val_dataset, args.combine, encoding=args.encoding)\n",
        "            else:\n",
        "                val_chunks = chunks\n",
        "        print('dataset has', data_sampler.total_size, 'tokens')\n",
        "        print('Training...')\n",
        "\n",
        "        if args.val_every > 0:\n",
        "            # Sample from validation set once with fixed seed to make\n",
        "            # it deterministic during training as well as across runs.\n",
        "            val_data_sampler = Sampler(val_chunks, seed=1)\n",
        "            val_batches = [[val_data_sampler.sample(1024) for _ in range(args.val_batch_size)]\n",
        "                           for _ in range(args.val_batch_count)]\n",
        "\n",
        "        counter = 1\n",
        "        counter_path = os.path.join(CHECKPOINT_DIR, args.run_name, 'counter')\n",
        "        if os.path.exists(counter_path):\n",
        "            # Load the step number if we're resuming a run\n",
        "            # Add 1 so we don't immediately try to save again\n",
        "            with open(counter_path, 'r') as fp:\n",
        "                counter = int(fp.read()) + 1\n",
        "\n",
        "        def save():\n",
        "            maketree(os.path.join(CHECKPOINT_DIR, args.run_name))\n",
        "            print(\n",
        "                'Saving',\n",
        "                os.path.join(CHECKPOINT_DIR, args.run_name,\n",
        "                             'model-{}').format(counter))\n",
        "            saver.save(\n",
        "                sess,\n",
        "                os.path.join(CHECKPOINT_DIR, args.run_name, 'model'),\n",
        "                global_step=counter)\n",
        "            with open(counter_path, 'w') as fp:\n",
        "                fp.write(str(counter) + '\\n')\n",
        "\n",
        "        def generate_samples():\n",
        "            print('Generating samples...')\n",
        "            context_tokens = data_sampler.sample(1)\n",
        "            all_text = []\n",
        "            index = 0\n",
        "            while index < args.sample_num:\n",
        "                out = sess.run(\n",
        "                    tf_sample,\n",
        "                    feed_dict={context: args.batch_size * [context_tokens]})\n",
        "                for i in range(min(args.sample_num - index, args.batch_size)):\n",
        "                    text = enc.decode(out[i])\n",
        "                    text = '======== SAMPLE {} ========\\n{}\\n'.format(\n",
        "                        index + 1, text)\n",
        "                    all_text.append(text)\n",
        "                    index += 1\n",
        "            print(text)\n",
        "            maketree(os.path.join(SAMPLE_DIR, args.run_name))\n",
        "            with open(\n",
        "                    os.path.join(SAMPLE_DIR, args.run_name,\n",
        "                                 'samples-{}').format(counter), 'w', encoding=args.encoding) as fp:\n",
        "                fp.write('\\n'.join(all_text))\n",
        "\n",
        "        def validation():\n",
        "            print('Calculating validation loss...')\n",
        "            losses = []\n",
        "            for batch in tqdm.tqdm(val_batches):\n",
        "                losses.append(sess.run(val_loss, feed_dict={val_context: batch}))\n",
        "            v_val_loss = np.mean(losses)\n",
        "            v_summary = sess.run(val_loss_summary, feed_dict={val_loss: v_val_loss})\n",
        "            summary_log.add_summary(v_summary, counter)\n",
        "            summary_log.flush()\n",
        "            print(\n",
        "                '[{counter} | {time:2.2f}] validation loss = {loss:2.2f}'\n",
        "                .format(\n",
        "                    counter=counter,\n",
        "                    time=time.time() - start_time,\n",
        "                    loss=v_val_loss))\n",
        "\n",
        "        def sample_batch():\n",
        "            return [data_sampler.sample(1024) for _ in range(args.batch_size)]\n",
        "\n",
        "\n",
        "        avg_loss = (0.0, 0.0)\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            while True:\n",
        "                if counter % args.save_every == 0:\n",
        "                    save()\n",
        "                if counter % args.sample_every == 0:\n",
        "                    generate_samples()\n",
        "                if args.val_every > 0 and (counter % args.val_every == 0 or counter == 1):\n",
        "                    validation()\n",
        "\n",
        "                if args.accumulate_gradients > 1:\n",
        "                    sess.run(opt_reset)\n",
        "                    for _ in range(args.accumulate_gradients):\n",
        "                        sess.run(\n",
        "                            opt_compute, feed_dict={context: sample_batch()})\n",
        "                    (v_loss, v_summary) = sess.run((opt_apply, summaries))\n",
        "                else:\n",
        "                    (_, v_loss, v_summary) = sess.run(\n",
        "                        (opt_apply, loss, summaries),\n",
        "                        feed_dict={context: sample_batch()})\n",
        "\n",
        "                summary_log.add_summary(v_summary, counter)\n",
        "\n",
        "                avg_loss = (avg_loss[0] * 0.99 + v_loss,\n",
        "                            avg_loss[1] * 0.99 + 1.0)\n",
        "\n",
        "                print(\n",
        "                    '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'\n",
        "                    .format(\n",
        "                        counter=counter,\n",
        "                        time=time.time() - start_time,\n",
        "                        loss=v_loss,\n",
        "                        avg=avg_loss[0] / avg_loss[1]))\n",
        "\n",
        "                counter += 1\n",
        "        except KeyboardInterrupt:\n",
        "            print('interrupted')\n",
        "            save()\n",
        "main(model_name, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e88s1g5IO9u",
        "colab_type": "text"
      },
      "source": [
        "## Test Model\n",
        "\n",
        "Change model directory name as needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLW0xtQ4FPgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir models/fine-tuned\n",
        "!cp /content/gdrive/My\\ Drive/models/run1/* models/fine-tuned/\n",
        "!cp models/ja-117M/{encoder.json,hparams.json,vocab.bpe} models/fine-tuned/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IqWvlF0G6Lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 gpt2-generate.py --model models/ja-117M --context=\"はじめまして\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}